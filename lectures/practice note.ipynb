{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6a94eb1d-501d-4374-b2dc-13b795c5e115",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5deab66a-5838-4af2-8c96-e28907cddcab",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(data={\"col1\": [0, 1], \"col2\": pd.Series([1], index=[1])}, index=[0, 1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "617c82b7-376a-4c65-8c8b-0c09d0a866a1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pandas.core.series.Series"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(df['col1'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "8a72166d-25c5-4ab1-b34c-b1f76a3ee398",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pandas.core.frame.DataFrame"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(df[['col1']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "9e29e48e-499d-4cd4-aac1-8479fba690aa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>col2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   col2\n",
       "0   NaN\n",
       "1   1.0"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[['col2']]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff11e08e-f2e1-4ac8-82a6-8f943a8bb102",
   "metadata": {},
   "source": [
    "## Count Vectorize Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4990305b-244d-44ed-8787-8f2c7d694761",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split, cross_validate, RandomizedSearchCV\n",
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler, OrdinalEncoder\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.compose import ColumnTransformer, make_column_transformer\n",
    "from sklearn.pipeline import Pipeline, make_pipeline\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "# Loading in the data\n",
    "tweets_df = pd.read_csv('data/balanced_tweets.csv').dropna(subset=['target'])\n",
    "\n",
    "# Split the dataset into the feature table `X` and the target value `y`\n",
    "X = tweets_df['text']\n",
    "y = tweets_df['target']\n",
    "\n",
    "# Split the dataset into X_train, X_test, y_train, y_test \n",
    "X_train, X_test, y_train, y_test = train_test_split(X,y, test_size=0.2, random_state=7)\n",
    "\n",
    "\n",
    "param_grid = {\n",
    "    \"countvectorizer__max_features\": range(1,1000)\n",
    "}\n",
    "\n",
    "# Make a pipeline with CountVectorizer as the first step and SVC as the second \n",
    "pipe = make_pipeline(CountVectorizer(), SVC())\n",
    "\n",
    "# perform RandomizedSearchCV using the parameters specified in param_grid\n",
    "# Don't forget to fit this on the training data\n",
    "tweet_search = RandomizedSearchCV(pipe, param_grid, n_jobs=-1, cv=5,\n",
    "                                  return_train_score=True, n_iter=10,\n",
    "                                   random_state=2020)\n",
    "tweet_search.fit(X_train, y_train)\n",
    "\n",
    "# What is the best max_features value? Save it in an object name tweet_feats\n",
    "tweet_feats = tweet_search.best_params_['countvectorizer__max_features']\n",
    "print(tweet_feats)\n",
    "\n",
    "# What is the best score? Save it in an object named tweet_val_score\n",
    "tweet_val_score = tweet_search.best_score_\n",
    "print(tweet_val_score)\n",
    "\n",
    "# Score the optimal model on the test set and save it in an object named tweet_test_score\n",
    "tweet_test_score = tweet_search.score(X_test, y_test)\n",
    "print(tweet_test_score)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f56e3b6-afff-49e3-8b30-05e84e258a75",
   "metadata": {},
   "source": [
    "## Multiclass Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36fc68d3-c5c2-47b6-b1a9-0e23839870a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import warnings\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)\n",
    "from sklearn.model_selection import train_test_split, cross_validate\n",
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler, OrdinalEncoder\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.compose import ColumnTransformer, make_column_transformer\n",
    "from sklearn.pipeline import Pipeline, make_pipeline\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import plot_confusion_matrix\n",
    "\n",
    "# Loading in the data\n",
    "bball = pd.read_csv('data/bball.csv')\n",
    "bball = bball[(bball['draft_year'] != 'Undrafted') & (bball['draft_round'] != 'Undrafted') & (bball['draft_peak'] != 'Undrafted')]\n",
    "bball = bball.replace({'F-G': 'Other', 'F-C': 'Other', 'G-F': 'Other', 'C-F': 'Other', 'C': 'Other'})\n",
    "df_train, df_test = train_test_split(bball, test_size=0.2, random_state=1)\n",
    "\n",
    "\n",
    "X_train = df_train[[\"weight\", \"height\", \"draft_year\", \"draft_round\",\n",
    "                     \"draft_peak\", \"team\", \"salary\", \"country\"]]\n",
    "X_test = df_test[[\"weight\", \"height\", \"draft_year\", \"draft_round\",\n",
    "                     \"draft_peak\", \"team\", \"salary\", \"country\"]]\n",
    "y_train = df_train['position']\n",
    "y_test = df_test['position']\n",
    "\n",
    "\n",
    "# Split the numeric and categorical features \n",
    "numeric_features = [ \"weight\",\n",
    "                     \"height\",\n",
    "                     \"draft_year\",\n",
    "                     \"draft_round\",\n",
    "                     \"draft_peak\"]\n",
    "\n",
    "categorical_features = [\"team\", \"country\"]\n",
    "\n",
    "\n",
    "# Build a numeric pipeline\n",
    "numeric_transformer = make_pipeline(\n",
    "    SimpleImputer(strategy=\"median\"))\n",
    "\n",
    "# Build a categorical pipeline\n",
    "categorical_transformer = make_pipeline(\n",
    "    SimpleImputer(strategy=\"most_frequent\"),\n",
    "    OneHotEncoder(handle_unknown=\"ignore\"))\n",
    "\n",
    "\n",
    "# Build a numeric pipeline\n",
    "numeric_transformer = make_pipeline(\n",
    "    SimpleImputer(strategy=\"median\"),\n",
    "    StandardScaler())\n",
    "\n",
    "# Build a categorical pipeline\n",
    "categorical_transformer = make_pipeline(\n",
    "    SimpleImputer(strategy=\"most_frequent\"),\n",
    "    OneHotEncoder(handle_unknown=\"ignore\"))\n",
    "\n",
    "# Build a categorical transformer\n",
    "col_transformer = make_column_transformer(\n",
    "    (numeric_transformer, numeric_features),\n",
    "    (categorical_transformer, categorical_features))\n",
    "\n",
    "# Build a main pipeline\n",
    "lr_pipe = make_pipeline(col_transformer, LogisticRegression())\n",
    "\n",
    "# Fit your pipeline on the training set\n",
    "lr_pipe.fit(X_train, y_train)\n",
    "\n",
    "# Plot your confusion matrix on your test set \n",
    "plot_confusion_matrix(lr_pipe, X_test, y_test)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:cpsc330]",
   "language": "python",
   "name": "conda-env-cpsc330-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
